2025-03-12 23:17:02,100	INFO worker.py:1841 -- Started a local Ray instance.
2025-03-12 23:17:06,063	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
2025-03-12 23:17:06,110	WARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[36m(train pid=2104629)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2104629)[0m The version_base parameter is not specified.
[36m(train pid=2104629)[0m Please specify a compatability version level, or None.
[36m(train pid=2104629)[0m Will assume defaults for version 1.1
[36m(train pid=2104629)[0m   with hydra.initialize(
[36m(train pid=2104629)[0m [rank: 0] Seed set to 0
[36m(train pid=2104629)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2104629)[0m GPU available: True (cuda), used: True
[36m(train pid=2104629)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2104629)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2104629)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2104629)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2104629)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/0_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=2,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=2104629)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2104629)[0m 
[36m(train pid=2104629)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2104629)[0m ----------------------------------------
[36m(train pid=2104629)[0m 0 | model | gNODE | 5.1 K  | train
[36m(train pid=2104629)[0m ----------------------------------------
[36m(train pid=2104629)[0m 5.1 K     Trainable params
[36m(train pid=2104629)[0m 0         Non-trainable params
[36m(train pid=2104629)[0m 5.1 K     Total params
[36m(train pid=2104629)[0m 0.021     Total estimated model params size (MB)
[36m(train pid=2104629)[0m 13        Modules in train mode
[36m(train pid=2104629)[0m 0         Modules in eval mode
[36m(train pid=2104629)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2104631)[0m 
[36m(train pid=2104632)[0m 
[36m(train pid=2104630)[0m 
[36m(train pid=2104629)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2104632)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2104630)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: [32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(train pid=2104630)[0m The version_base parameter is not specified.[32m [repeated 7x across cluster][0m
[36m(train pid=2104630)[0m Please specify a compatability version level, or None.[32m [repeated 7x across cluster][0m
[36m(train pid=2104630)[0m Will assume defaults for version 1.1[32m [repeated 7x across cluster][0m
[36m(train pid=2104630)[0m   with hydra.initialize([32m [repeated 7x across cluster][0m
[36m(train pid=2104630)[0m [rank: 0] Seed set to 0[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m GPU available: True (cuda), used: True[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m TPU available: False, using: 0 TPU cores[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m HPU available: False, using: 0 HPUs[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved![32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/3_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0100 exists and is not empty.[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m   | Name  | Type  | Params | Mode [32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m ----------------------------------------[32m [repeated 6x across cluster][0m
[36m(train pid=2104630)[0m 0 | model | gNODE | 6.2 K  | train[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 6.2 K     Trainable params[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 0         Non-trainable params[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 6.2 K     Total params[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 0.025     Total estimated model params size (MB)[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 15        Modules in train mode[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m 0         Modules in eval mode[32m [repeated 3x across cluster][0m
[36m(train pid=2104630)[0m SLURM auto-requeueing enabled. Setting signal handlers.[32m [repeated 3x across cluster][0m
[36m(train pid=2104632)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 3x across cluster][0m
[36m(train pid=2253860)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2253860)[0m The version_base parameter is not specified.
[36m(train pid=2253860)[0m Please specify a compatability version level, or None.
[36m(train pid=2253860)[0m Will assume defaults for version 1.1
[36m(train pid=2253860)[0m   with hydra.initialize(
[36m(train pid=2253860)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2253860)[0m The version_base parameter is not specified.
[36m(train pid=2253860)[0m Please specify a compatability version level, or None.
[36m(train pid=2253860)[0m Will assume defaults for version 1.1
[36m(train pid=2253860)[0m   with hydra.initialize(
[36m(train pid=2253860)[0m [rank: 0] Seed set to 0
[36m(train pid=2253860)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2253860)[0m GPU available: True (cuda), used: True
[36m(train pid=2253860)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2253860)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2253860)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2253860)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2253860)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/4_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=2253860)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2253860)[0m 
[36m(train pid=2253860)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2253860)[0m ----------------------------------------
[36m(train pid=2253860)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=2253860)[0m ----------------------------------------
[36m(train pid=2253860)[0m 6.2 K     Trainable params
[36m(train pid=2253860)[0m 0         Non-trainable params
[36m(train pid=2253860)[0m 6.2 K     Total params
[36m(train pid=2253860)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=2253860)[0m 15        Modules in train mode
[36m(train pid=2253860)[0m 0         Modules in eval mode
[36m(train pid=2253860)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2253860)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2104631)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2258627)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2258627)[0m The version_base parameter is not specified.
[36m(train pid=2258627)[0m Please specify a compatability version level, or None.
[36m(train pid=2258627)[0m Will assume defaults for version 1.1
[36m(train pid=2258627)[0m   with hydra.initialize(
[36m(train pid=2258627)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2258627)[0m The version_base parameter is not specified.
[36m(train pid=2258627)[0m Please specify a compatability version level, or None.
[36m(train pid=2258627)[0m Will assume defaults for version 1.1
[36m(train pid=2258627)[0m   with hydra.initialize(
[36m(train pid=2258627)[0m [rank: 0] Seed set to 0
[36m(train pid=2258627)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2258627)[0m GPU available: True (cuda), used: True
[36m(train pid=2258627)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2258627)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2258627)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2258627)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2258627)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/5_env_params_noise=0.0000,model_gating_hidden_size=64,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=2258627)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2258627)[0m 
[36m(train pid=2258627)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2258627)[0m ----------------------------------------
[36m(train pid=2258627)[0m 0 | model | gNODE | 5.5 K  | train
[36m(train pid=2258627)[0m ----------------------------------------
[36m(train pid=2258627)[0m 5.5 K     Trainable params
[36m(train pid=2258627)[0m 0         Non-trainable params
[36m(train pid=2258627)[0m 5.5 K     Total params
[36m(train pid=2258627)[0m 0.022     Total estimated model params size (MB)
[36m(train pid=2258627)[0m 13        Modules in train mode
[36m(train pid=2258627)[0m 0         Modules in eval mode
[36m(train pid=2258627)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2258627)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2104629)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2263008)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2263008)[0m The version_base parameter is not specified.
[36m(train pid=2263008)[0m Please specify a compatability version level, or None.
[36m(train pid=2263008)[0m Will assume defaults for version 1.1
[36m(train pid=2263008)[0m   with hydra.initialize(
[36m(train pid=2263008)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2263008)[0m The version_base parameter is not specified.
[36m(train pid=2263008)[0m Please specify a compatability version level, or None.
[36m(train pid=2263008)[0m Will assume defaults for version 1.1
[36m(train pid=2263008)[0m   with hydra.initialize(
[36m(train pid=2263008)[0m [rank: 0] Seed set to 0
[36m(train pid=2263008)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2263008)[0m GPU available: True (cuda), used: True
[36m(train pid=2263008)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2263008)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2263008)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2263008)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2263008)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/6_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=2263008)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2263008)[0m 
[36m(train pid=2263008)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2263008)[0m ----------------------------------------
[36m(train pid=2263008)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=2263008)[0m ----------------------------------------
[36m(train pid=2263008)[0m 6.2 K     Trainable params
[36m(train pid=2263008)[0m 0         Non-trainable params
[36m(train pid=2263008)[0m 6.2 K     Total params
[36m(train pid=2263008)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=2263008)[0m 15        Modules in train mode
[36m(train pid=2263008)[0m 0         Modules in eval mode
[36m(train pid=2263008)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2263008)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2104630)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2289311)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2289311)[0m The version_base parameter is not specified.
[36m(train pid=2289311)[0m Please specify a compatability version level, or None.
[36m(train pid=2289311)[0m Will assume defaults for version 1.1
[36m(train pid=2289311)[0m   with hydra.initialize(
[36m(train pid=2289311)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2289311)[0m The version_base parameter is not specified.
[36m(train pid=2289311)[0m Please specify a compatability version level, or None.
[36m(train pid=2289311)[0m Will assume defaults for version 1.1
[36m(train pid=2289311)[0m   with hydra.initialize(
[36m(train pid=2289311)[0m [rank: 0] Seed set to 0
[36m(train pid=2289311)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2289311)[0m GPU available: True (cuda), used: True
[36m(train pid=2289311)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2289311)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2289311)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2289311)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2289311)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/7_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=2289311)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2289311)[0m 
[36m(train pid=2289311)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2289311)[0m ----------------------------------------
[36m(train pid=2289311)[0m 0 | model | gNODE | 5.1 K  | train
[36m(train pid=2289311)[0m ----------------------------------------
[36m(train pid=2289311)[0m 5.1 K     Trainable params
[36m(train pid=2289311)[0m 0         Non-trainable params
[36m(train pid=2289311)[0m 5.1 K     Total params
[36m(train pid=2289311)[0m 0.021     Total estimated model params size (MB)
[36m(train pid=2289311)[0m 13        Modules in train mode
[36m(train pid=2289311)[0m 0         Modules in eval mode
[36m(train pid=2289311)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2289311)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2258627)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2400416)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2400416)[0m The version_base parameter is not specified.
[36m(train pid=2400416)[0m Please specify a compatability version level, or None.
[36m(train pid=2400416)[0m Will assume defaults for version 1.1
[36m(train pid=2400416)[0m   with hydra.initialize(
[36m(train pid=2400416)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2400416)[0m The version_base parameter is not specified.
[36m(train pid=2400416)[0m Please specify a compatability version level, or None.
[36m(train pid=2400416)[0m Will assume defaults for version 1.1
[36m(train pid=2400416)[0m   with hydra.initialize(
[36m(train pid=2400416)[0m [rank: 0] Seed set to 0
[36m(train pid=2400416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2400416)[0m GPU available: True (cuda), used: True
[36m(train pid=2400416)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2400416)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2400416)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2400416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2400416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/8_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=2400416)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2400416)[0m 
[36m(train pid=2400416)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2400416)[0m ----------------------------------------
[36m(train pid=2400416)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=2400416)[0m ----------------------------------------
[36m(train pid=2400416)[0m 6.2 K     Trainable params
[36m(train pid=2400416)[0m 0         Non-trainable params
[36m(train pid=2400416)[0m 6.2 K     Total params
[36m(train pid=2400416)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=2400416)[0m 15        Modules in train mode
[36m(train pid=2400416)[0m 0         Modules in eval mode
[36m(train pid=2400416)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2400416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2253860)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2432708)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2432708)[0m The version_base parameter is not specified.
[36m(train pid=2432708)[0m Please specify a compatability version level, or None.
[36m(train pid=2432708)[0m Will assume defaults for version 1.1
[36m(train pid=2432708)[0m   with hydra.initialize(
[36m(train pid=2432708)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2432708)[0m The version_base parameter is not specified.
[36m(train pid=2432708)[0m Please specify a compatability version level, or None.
[36m(train pid=2432708)[0m Will assume defaults for version 1.1
[36m(train pid=2432708)[0m   with hydra.initialize(
[36m(train pid=2432708)[0m [rank: 0] Seed set to 0
[36m(train pid=2432708)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2432708)[0m GPU available: True (cuda), used: True
[36m(train pid=2432708)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2432708)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2432708)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2432708)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2432708)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/9_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=2432708)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2432708)[0m 
[36m(train pid=2432708)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2432708)[0m ----------------------------------------
[36m(train pid=2432708)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=2432708)[0m ----------------------------------------
[36m(train pid=2432708)[0m 6.2 K     Trainable params
[36m(train pid=2432708)[0m 0         Non-trainable params
[36m(train pid=2432708)[0m 6.2 K     Total params
[36m(train pid=2432708)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=2432708)[0m 15        Modules in train mode
[36m(train pid=2432708)[0m 0         Modules in eval mode
[36m(train pid=2432708)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2432708)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2289311)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2435255)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2435255)[0m The version_base parameter is not specified.
[36m(train pid=2435255)[0m Please specify a compatability version level, or None.
[36m(train pid=2435255)[0m Will assume defaults for version 1.1
[36m(train pid=2435255)[0m   with hydra.initialize(
[36m(train pid=2435255)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2435255)[0m The version_base parameter is not specified.
[36m(train pid=2435255)[0m Please specify a compatability version level, or None.
[36m(train pid=2435255)[0m Will assume defaults for version 1.1
[36m(train pid=2435255)[0m   with hydra.initialize(
[36m(train pid=2435255)[0m [rank: 0] Seed set to 0
[36m(train pid=2435255)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2435255)[0m GPU available: True (cuda), used: True
[36m(train pid=2435255)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2435255)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2435255)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2435255)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2435255)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/10_env_params_noise=0.0000,model_gating_hidden_size=64,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=2435255)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2435255)[0m 
[36m(train pid=2435255)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2435255)[0m ----------------------------------------
[36m(train pid=2435255)[0m 0 | model | gNODE | 5.5 K  | train
[36m(train pid=2435255)[0m ----------------------------------------
[36m(train pid=2435255)[0m 5.5 K     Trainable params
[36m(train pid=2435255)[0m 0         Non-trainable params
[36m(train pid=2435255)[0m 5.5 K     Total params
[36m(train pid=2435255)[0m 0.022     Total estimated model params size (MB)
[36m(train pid=2435255)[0m 13        Modules in train mode
[36m(train pid=2435255)[0m 0         Modules in eval mode
[36m(train pid=2435255)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2435255)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=2263008)[0m `Trainer.fit` stopped: `max_epochs=1000` reached.
[36m(train pid=2442060)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2442060)[0m The version_base parameter is not specified.
[36m(train pid=2442060)[0m Please specify a compatability version level, or None.
[36m(train pid=2442060)[0m Will assume defaults for version 1.1
[36m(train pid=2442060)[0m   with hydra.initialize(
[36m(train pid=2442060)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=2442060)[0m The version_base parameter is not specified.
[36m(train pid=2442060)[0m Please specify a compatability version level, or None.
[36m(train pid=2442060)[0m Will assume defaults for version 1.1
[36m(train pid=2442060)[0m   with hydra.initialize(
[36m(train pid=2442060)[0m [rank: 0] Seed set to 0
[36m(train pid=2442060)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=2442060)[0m GPU available: True (cuda), used: True
[36m(train pid=2442060)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=2442060)[0m HPU available: False, using: 0 HPUs
[36m(train pid=2442060)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=2442060)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=2442060)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_23-17-00_669869_2101465/artifacts/2025-03-12_23-17-06/train_2025-03-12_23-17-06/working_dirs/11_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=2442060)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=2442060)[0m 
[36m(train pid=2442060)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=2442060)[0m ----------------------------------------
[36m(train pid=2442060)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=2442060)[0m ----------------------------------------
[36m(train pid=2442060)[0m 6.2 K     Trainable params
[36m(train pid=2442060)[0m 0         Non-trainable params
[36m(train pid=2442060)[0m 6.2 K     Total params
[36m(train pid=2442060)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=2442060)[0m 15        Modules in train mode
[36m(train pid=2442060)[0m 0         Modules in eval mode
[36m(train pid=2442060)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=2442060)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
slurmstepd: error: *** JOB 62868431 ON della-l01g15 CANCELLED AT 2025-03-13T07:21:57 DUE TO TIME LIMIT ***
