2025-03-12 15:30:12,043	INFO worker.py:1841 -- Started a local Ray instance.
2025-03-12 15:30:15,891	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
2025-03-12 15:30:15,902	WARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[36m(train pid=3392097)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3392097)[0m The version_base parameter is not specified.
[36m(train pid=3392097)[0m Please specify a compatability version level, or None.
[36m(train pid=3392097)[0m Will assume defaults for version 1.1
[36m(train pid=3392097)[0m   with hydra.initialize(
[36m(train pid=3392097)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3392097)[0m The version_base parameter is not specified.
[36m(train pid=3392097)[0m Please specify a compatability version level, or None.
[36m(train pid=3392097)[0m Will assume defaults for version 1.1
[36m(train pid=3392097)[0m   with hydra.initialize(
[36m(train pid=3392097)[0m [rank: 0] Seed set to 0
[36m(train pid=3392097)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3392097)[0m GPU available: True (cuda), used: True
[36m(train pid=3392097)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3392097)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3392098)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3392097)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3392097)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/1_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=3392097)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3392098)[0m 
[36m(train pid=3392098)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3392098)[0m ----------------------------------------
[36m(train pid=3392098)[0m 0 | model | gNODE | 5.1 K  | train
[36m(train pid=3392098)[0m ----------------------------------------
[36m(train pid=3392098)[0m 5.1 K     Trainable params
[36m(train pid=3392098)[0m 0         Non-trainable params
[36m(train pid=3392098)[0m 5.1 K     Total params
[36m(train pid=3392098)[0m 0.021     Total estimated model params size (MB)
[36m(train pid=3392098)[0m 13        Modules in train mode
[36m(train pid=3392098)[0m 0         Modules in eval mode
[36m(train pid=3392098)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3392097)[0m 
[36m(train pid=3392096)[0m 
[36m(train pid=3392099)[0m 
[36m(train pid=3392098)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3392099)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: [32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(train pid=3392099)[0m The version_base parameter is not specified.[32m [repeated 6x across cluster][0m
[36m(train pid=3392099)[0m Please specify a compatability version level, or None.[32m [repeated 6x across cluster][0m
[36m(train pid=3392099)[0m Will assume defaults for version 1.1[32m [repeated 6x across cluster][0m
[36m(train pid=3392099)[0m   with hydra.initialize([32m [repeated 6x across cluster][0m
[36m(train pid=3392099)[0m [rank: 0] Seed set to 0[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m GPU available: True (cuda), used: True[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m TPU available: False, using: 0 TPU cores[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m HPU available: False, using: 0 HPUs[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision[32m [repeated 3x across cluster][0m
[36m(train pid=3392098)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved![32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/3_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m   | Name  | Type  | Params | Mode [32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m ----------------------------------------[32m [repeated 6x across cluster][0m
[36m(train pid=3392099)[0m 0 | model | gNODE | 5.1 K  | train[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 5.1 K     Trainable params[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 0         Non-trainable params[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 5.1 K     Total params[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 0.021     Total estimated model params size (MB)[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 13        Modules in train mode[32m [repeated 3x across cluster][0m
[36m(train pid=3392099)[0m 0         Modules in eval mode[32m [repeated 3x across cluster][0m
[36m(train pid=3392097)[0m SLURM auto-requeueing enabled. Setting signal handlers.[32m [repeated 3x across cluster][0m
[36m(train pid=3392097)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 3x across cluster][0m
[36m(train pid=3393472)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: [32m [repeated 2x across cluster][0m
[36m(train pid=3393472)[0m The version_base parameter is not specified.[32m [repeated 2x across cluster][0m
[36m(train pid=3393472)[0m Please specify a compatability version level, or None.[32m [repeated 2x across cluster][0m
[36m(train pid=3393472)[0m Will assume defaults for version 1.1[32m [repeated 2x across cluster][0m
[36m(train pid=3393472)[0m   with hydra.initialize([32m [repeated 2x across cluster][0m
[36m(train pid=3393472)[0m [rank: 0] Seed set to 0
[36m(train pid=3393472)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3393472)[0m GPU available: True (cuda), used: True
[36m(train pid=3393472)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3393472)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3393472)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3393472)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3393472)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/4_env_params_noise=0.1000,model_gating_hidden_size=64,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3393472)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3393472)[0m 
[36m(train pid=3393472)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3393472)[0m ----------------------------------------
[36m(train pid=3393472)[0m 0 | model | gNODE | 9.6 K  | train
[36m(train pid=3393472)[0m ----------------------------------------
[36m(train pid=3393472)[0m 9.6 K     Trainable params
[36m(train pid=3393472)[0m 0         Non-trainable params
[36m(train pid=3393472)[0m 9.6 K     Total params
[36m(train pid=3393472)[0m 0.038     Total estimated model params size (MB)
[36m(train pid=3393472)[0m 15        Modules in train mode
[36m(train pid=3393472)[0m 0         Modules in eval mode
[36m(train pid=3393472)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3393472)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3394169)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3394169)[0m The version_base parameter is not specified.
[36m(train pid=3394169)[0m Please specify a compatability version level, or None.
[36m(train pid=3394169)[0m Will assume defaults for version 1.1
[36m(train pid=3394169)[0m   with hydra.initialize(
[36m(train pid=3394169)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3394169)[0m The version_base parameter is not specified.
[36m(train pid=3394169)[0m Please specify a compatability version level, or None.
[36m(train pid=3394169)[0m Will assume defaults for version 1.1
[36m(train pid=3394169)[0m   with hydra.initialize(
[36m(train pid=3394169)[0m [rank: 0] Seed set to 0
[36m(train pid=3394169)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3394169)[0m GPU available: True (cuda), used: True
[36m(train pid=3394169)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3394169)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3394169)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3394169)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3394169)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/5_env_params_noise=0.1000,model_gating_hidden_size=64,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=3394169)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3394169)[0m 
[36m(train pid=3394169)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3394169)[0m ----------------------------------------
[36m(train pid=3394169)[0m 0 | model | gNODE | 5.5 K  | train
[36m(train pid=3394169)[0m ----------------------------------------
[36m(train pid=3394169)[0m 5.5 K     Trainable params
[36m(train pid=3394169)[0m 0         Non-trainable params
[36m(train pid=3394169)[0m 5.5 K     Total params
[36m(train pid=3394169)[0m 0.022     Total estimated model params size (MB)
[36m(train pid=3394169)[0m 13        Modules in train mode
[36m(train pid=3394169)[0m 0         Modules in eval mode
[36m(train pid=3394169)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3394169)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3394795)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3394795)[0m The version_base parameter is not specified.
[36m(train pid=3394795)[0m Please specify a compatability version level, or None.
[36m(train pid=3394795)[0m Will assume defaults for version 1.1
[36m(train pid=3394795)[0m   with hydra.initialize(
[36m(train pid=3394795)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3394795)[0m The version_base parameter is not specified.
[36m(train pid=3394795)[0m Please specify a compatability version level, or None.
[36m(train pid=3394795)[0m Will assume defaults for version 1.1
[36m(train pid=3394795)[0m   with hydra.initialize(
[36m(train pid=3394795)[0m [rank: 0] Seed set to 0
[36m(train pid=3394795)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3394795)[0m GPU available: True (cuda), used: True
[36m(train pid=3394795)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3394795)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3394795)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3394795)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3394795)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/6_env_params_noise=0.0000,model_gating_hidden_size=64,model_gating_num_layers=2,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3394795)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3394795)[0m 
[36m(train pid=3394795)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3394795)[0m ----------------------------------------
[36m(train pid=3394795)[0m 0 | model | gNODE | 5.5 K  | train
[36m(train pid=3394795)[0m ----------------------------------------
[36m(train pid=3394795)[0m 5.5 K     Trainable params
[36m(train pid=3394795)[0m 0         Non-trainable params
[36m(train pid=3394795)[0m 5.5 K     Total params
[36m(train pid=3394795)[0m 0.022     Total estimated model params size (MB)
[36m(train pid=3394795)[0m 13        Modules in train mode
[36m(train pid=3394795)[0m 0         Modules in eval mode
[36m(train pid=3394795)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3394795)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3395416)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3395416)[0m The version_base parameter is not specified.
[36m(train pid=3395416)[0m Please specify a compatability version level, or None.
[36m(train pid=3395416)[0m Will assume defaults for version 1.1
[36m(train pid=3395416)[0m   with hydra.initialize(
[36m(train pid=3395416)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3395416)[0m The version_base parameter is not specified.
[36m(train pid=3395416)[0m Please specify a compatability version level, or None.
[36m(train pid=3395416)[0m Will assume defaults for version 1.1
[36m(train pid=3395416)[0m   with hydra.initialize(
[36m(train pid=3395416)[0m [rank: 0] Seed set to 0
[36m(train pid=3395416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3395416)[0m GPU available: True (cuda), used: True
[36m(train pid=3395416)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3395416)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3395416)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3395416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3395416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/7_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3395416)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3395416)[0m 
[36m(train pid=3395416)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3395416)[0m ----------------------------------------
[36m(train pid=3395416)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=3395416)[0m ----------------------------------------
[36m(train pid=3395416)[0m 6.2 K     Trainable params
[36m(train pid=3395416)[0m 0         Non-trainable params
[36m(train pid=3395416)[0m 6.2 K     Total params
[36m(train pid=3395416)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=3395416)[0m 15        Modules in train mode
[36m(train pid=3395416)[0m 0         Modules in eval mode
[36m(train pid=3395416)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3395416)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3396281)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3396281)[0m The version_base parameter is not specified.
[36m(train pid=3396281)[0m Please specify a compatability version level, or None.
[36m(train pid=3396281)[0m Will assume defaults for version 1.1
[36m(train pid=3396281)[0m   with hydra.initialize(
[36m(train pid=3396281)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3396281)[0m The version_base parameter is not specified.
[36m(train pid=3396281)[0m Please specify a compatability version level, or None.
[36m(train pid=3396281)[0m Will assume defaults for version 1.1
[36m(train pid=3396281)[0m   with hydra.initialize(
[36m(train pid=3396281)[0m [rank: 0] Seed set to 0
[36m(train pid=3396281)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3396281)[0m GPU available: True (cuda), used: True
[36m(train pid=3396281)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3396281)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3396281)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3396281)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3396281)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/8_env_params_noise=0.1000,model_gating_hidden_size=64,model_gating_num_layers=2,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=3396281)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3396281)[0m 
[36m(train pid=3396281)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3396281)[0m ----------------------------------------
[36m(train pid=3396281)[0m 0 | model | gNODE | 5.5 K  | train
[36m(train pid=3396281)[0m ----------------------------------------
[36m(train pid=3396281)[0m 5.5 K     Trainable params
[36m(train pid=3396281)[0m 0         Non-trainable params
[36m(train pid=3396281)[0m 5.5 K     Total params
[36m(train pid=3396281)[0m 0.022     Total estimated model params size (MB)
[36m(train pid=3396281)[0m 13        Modules in train mode
[36m(train pid=3396281)[0m 0         Modules in eval mode
[36m(train pid=3396281)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3396281)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3396899)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3396899)[0m The version_base parameter is not specified.
[36m(train pid=3396899)[0m Please specify a compatability version level, or None.
[36m(train pid=3396899)[0m Will assume defaults for version 1.1
[36m(train pid=3396899)[0m   with hydra.initialize(
[36m(train pid=3396899)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3396899)[0m The version_base parameter is not specified.
[36m(train pid=3396899)[0m Please specify a compatability version level, or None.
[36m(train pid=3396899)[0m Will assume defaults for version 1.1
[36m(train pid=3396899)[0m   with hydra.initialize(
[36m(train pid=3396899)[0m [rank: 0] Seed set to 0
[36m(train pid=3396899)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3396899)[0m GPU available: True (cuda), used: True
[36m(train pid=3396899)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3396899)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3396899)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3396899)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3396899)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/9_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=2,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3396899)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3396899)[0m 
[36m(train pid=3396899)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3396899)[0m ----------------------------------------
[36m(train pid=3396899)[0m 0 | model | gNODE | 5.1 K  | train
[36m(train pid=3396899)[0m ----------------------------------------
[36m(train pid=3396899)[0m 5.1 K     Trainable params
[36m(train pid=3396899)[0m 0         Non-trainable params
[36m(train pid=3396899)[0m 5.1 K     Total params
[36m(train pid=3396899)[0m 0.021     Total estimated model params size (MB)
[36m(train pid=3396899)[0m 13        Modules in train mode
[36m(train pid=3396899)[0m 0         Modules in eval mode
[36m(train pid=3396899)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3396899)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3397606)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3397606)[0m The version_base parameter is not specified.
[36m(train pid=3397606)[0m Please specify a compatability version level, or None.
[36m(train pid=3397606)[0m Will assume defaults for version 1.1
[36m(train pid=3397606)[0m   with hydra.initialize(
[36m(train pid=3397606)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3397606)[0m The version_base parameter is not specified.
[36m(train pid=3397606)[0m Please specify a compatability version level, or None.
[36m(train pid=3397606)[0m Will assume defaults for version 1.1
[36m(train pid=3397606)[0m   with hydra.initialize(
[36m(train pid=3397606)[0m [rank: 0] Seed set to 0
[36m(train pid=3397606)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3397606)[0m GPU available: True (cuda), used: True
[36m(train pid=3397606)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3397606)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3397606)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3397606)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3397606)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/10_env_params_noise=0.1000,model_gating_hidden_size=64,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3397606)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3397606)[0m 
[36m(train pid=3397606)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3397606)[0m ----------------------------------------
[36m(train pid=3397606)[0m 0 | model | gNODE | 9.6 K  | train
[36m(train pid=3397606)[0m ----------------------------------------
[36m(train pid=3397606)[0m 9.6 K     Trainable params
[36m(train pid=3397606)[0m 0         Non-trainable params
[36m(train pid=3397606)[0m 9.6 K     Total params
[36m(train pid=3397606)[0m 0.038     Total estimated model params size (MB)
[36m(train pid=3397606)[0m 15        Modules in train mode
[36m(train pid=3397606)[0m 0         Modules in eval mode
[36m(train pid=3397606)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3397606)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3398445)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3398445)[0m The version_base parameter is not specified.
[36m(train pid=3398445)[0m Please specify a compatability version level, or None.
[36m(train pid=3398445)[0m Will assume defaults for version 1.1
[36m(train pid=3398445)[0m   with hydra.initialize(
[36m(train pid=3398445)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3398445)[0m The version_base parameter is not specified.
[36m(train pid=3398445)[0m Please specify a compatability version level, or None.
[36m(train pid=3398445)[0m Will assume defaults for version 1.1
[36m(train pid=3398445)[0m   with hydra.initialize(
[36m(train pid=3398445)[0m [rank: 0] Seed set to 0
[36m(train pid=3398445)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3398445)[0m GPU available: True (cuda), used: True
[36m(train pid=3398445)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3398445)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3398445)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3398445)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3398445)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/11_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=3398445)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3398445)[0m 
[36m(train pid=3398445)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3398445)[0m ----------------------------------------
[36m(train pid=3398445)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=3398445)[0m ----------------------------------------
[36m(train pid=3398445)[0m 6.2 K     Trainable params
[36m(train pid=3398445)[0m 0         Non-trainable params
[36m(train pid=3398445)[0m 6.2 K     Total params
[36m(train pid=3398445)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=3398445)[0m 15        Modules in train mode
[36m(train pid=3398445)[0m 0         Modules in eval mode
[36m(train pid=3398445)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3398445)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3399579)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3399579)[0m The version_base parameter is not specified.
[36m(train pid=3399579)[0m Please specify a compatability version level, or None.
[36m(train pid=3399579)[0m Will assume defaults for version 1.1
[36m(train pid=3399579)[0m   with hydra.initialize(
[36m(train pid=3399579)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3399579)[0m The version_base parameter is not specified.
[36m(train pid=3399579)[0m Please specify a compatability version level, or None.
[36m(train pid=3399579)[0m Will assume defaults for version 1.1
[36m(train pid=3399579)[0m   with hydra.initialize(
[36m(train pid=3399579)[0m [rank: 0] Seed set to 0
[36m(train pid=3399579)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3399579)[0m GPU available: True (cuda), used: True
[36m(train pid=3399579)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3399579)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3399579)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3399579)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3399579)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/12_env_params_noise=0.1000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3399579)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3399579)[0m 
[36m(train pid=3399579)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3399579)[0m ----------------------------------------
[36m(train pid=3399579)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=3399579)[0m ----------------------------------------
[36m(train pid=3399579)[0m 6.2 K     Trainable params
[36m(train pid=3399579)[0m 0         Non-trainable params
[36m(train pid=3399579)[0m 6.2 K     Total params
[36m(train pid=3399579)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=3399579)[0m 15        Modules in train mode
[36m(train pid=3399579)[0m 0         Modules in eval mode
[36m(train pid=3399579)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3399579)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3400250)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3400250)[0m The version_base parameter is not specified.
[36m(train pid=3400250)[0m Please specify a compatability version level, or None.
[36m(train pid=3400250)[0m Will assume defaults for version 1.1
[36m(train pid=3400250)[0m   with hydra.initialize(
[36m(train pid=3400250)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3400250)[0m The version_base parameter is not specified.
[36m(train pid=3400250)[0m Please specify a compatability version level, or None.
[36m(train pid=3400250)[0m Will assume defaults for version 1.1
[36m(train pid=3400250)[0m   with hydra.initialize(
[36m(train pid=3400250)[0m [rank: 0] Seed set to 0
[36m(train pid=3400250)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3400250)[0m GPU available: True (cuda), used: True
[36m(train pid=3400250)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3400250)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3400250)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3400250)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3400250)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/13_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=2,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3400250)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3400250)[0m 
[36m(train pid=3400250)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3400250)[0m ----------------------------------------
[36m(train pid=3400250)[0m 0 | model | gNODE | 5.1 K  | train
[36m(train pid=3400250)[0m ----------------------------------------
[36m(train pid=3400250)[0m 5.1 K     Trainable params
[36m(train pid=3400250)[0m 0         Non-trainable params
[36m(train pid=3400250)[0m 5.1 K     Total params
[36m(train pid=3400250)[0m 0.021     Total estimated model params size (MB)
[36m(train pid=3400250)[0m 13        Modules in train mode
[36m(train pid=3400250)[0m 0         Modules in eval mode
[36m(train pid=3400250)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3400250)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3400887)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3400887)[0m The version_base parameter is not specified.
[36m(train pid=3400887)[0m Please specify a compatability version level, or None.
[36m(train pid=3400887)[0m Will assume defaults for version 1.1
[36m(train pid=3400887)[0m   with hydra.initialize(
[36m(train pid=3400887)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3400887)[0m The version_base parameter is not specified.
[36m(train pid=3400887)[0m Please specify a compatability version level, or None.
[36m(train pid=3400887)[0m Will assume defaults for version 1.1
[36m(train pid=3400887)[0m   with hydra.initialize(
[36m(train pid=3400887)[0m [rank: 0] Seed set to 0
[36m(train pid=3400887)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3400887)[0m GPU available: True (cuda), used: True
[36m(train pid=3400887)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3400887)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3400887)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3400887)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3400887)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/14_env_params_noise=0.0000,model_gating_hidden_size=32,model_gating_num_layers=3,task_wrapper_learning_rate=0.0010 exists and is not empty.
[36m(train pid=3400887)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3400887)[0m 
[36m(train pid=3400887)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3400887)[0m ----------------------------------------
[36m(train pid=3400887)[0m 0 | model | gNODE | 6.2 K  | train
[36m(train pid=3400887)[0m ----------------------------------------
[36m(train pid=3400887)[0m 6.2 K     Trainable params
[36m(train pid=3400887)[0m 0         Non-trainable params
[36m(train pid=3400887)[0m 6.2 K     Total params
[36m(train pid=3400887)[0m 0.025     Total estimated model params size (MB)
[36m(train pid=3400887)[0m 15        Modules in train mode
[36m(train pid=3400887)[0m 0         Modules in eval mode
[36m(train pid=3400887)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3400887)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(train pid=3401702)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3401702)[0m The version_base parameter is not specified.
[36m(train pid=3401702)[0m Please specify a compatability version level, or None.
[36m(train pid=3401702)[0m Will assume defaults for version 1.1
[36m(train pid=3401702)[0m   with hydra.initialize(
[36m(train pid=3401702)[0m /home/ad2002/ComputationThruDynamicsBenchmark/ctd/task_modeling/task_training.py:69: UserWarning: 
[36m(train pid=3401702)[0m The version_base parameter is not specified.
[36m(train pid=3401702)[0m Please specify a compatability version level, or None.
[36m(train pid=3401702)[0m Will assume defaults for version 1.1
[36m(train pid=3401702)[0m   with hydra.initialize(
[36m(train pid=3401702)[0m [rank: 0] Seed set to 0
[36m(train pid=3401702)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.
[36m(train pid=3401702)[0m GPU available: True (cuda), used: True
[36m(train pid=3401702)[0m TPU available: False, using: 0 TPU cores
[36m(train pid=3401702)[0m HPU available: False, using: 0 HPUs
[36m(train pid=3401702)[0m You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[36m(train pid=3401702)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
[36m(train pid=3401702)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /tmp/ray/session_2025-03-12_15-30-09_972939_3388950/artifacts/2025-03-12_15-30-15/train_2025-03-12_15-30-15/working_dirs/15_env_params_noise=0.0000,model_gating_hidden_size=64,model_gating_num_layers=3,task_wrapper_learning_rate=0.0100 exists and is not empty.
[36m(train pid=3401702)[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[36m(train pid=3401702)[0m 
[36m(train pid=3401702)[0m   | Name  | Type  | Params | Mode 
[36m(train pid=3401702)[0m ----------------------------------------
[36m(train pid=3401702)[0m 0 | model | gNODE | 9.6 K  | train
[36m(train pid=3401702)[0m ----------------------------------------
[36m(train pid=3401702)[0m 9.6 K     Trainable params
[36m(train pid=3401702)[0m 0         Non-trainable params
[36m(train pid=3401702)[0m 9.6 K     Total params
[36m(train pid=3401702)[0m 0.038     Total estimated model params size (MB)
[36m(train pid=3401702)[0m 15        Modules in train mode
[36m(train pid=3401702)[0m 0         Modules in eval mode
[36m(train pid=3401702)[0m SLURM auto-requeueing enabled. Setting signal handlers.
[36m(train pid=3401702)[0m /home/ad2002/.conda/envs/ctd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
slurmstepd: error: *** JOB 62867811 ON della-l04g1 CANCELLED AT 2025-03-12T15:47:05 ***
